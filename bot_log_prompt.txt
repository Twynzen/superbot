A VER ESTE ES EL PROYECTO ENTIENDELO MUY BIEN ANTES DE CUALQUIER COSA:

El proyecto se divide en estas carpetas
API: Tiene 3 archivos aun en desarrollo esta sección: 

bot_controller.py:
from api.language_model import get_gpt_response
from modules.resource_management import search_resources
from modules.combat import searchMob
from modules.characters_tracking import click_on_character

def execute_action(action):
    if action == 'collect_resources':
        search_resources()
    elif action == 'search_mob':
        searchMob()
    elif action == 'track_character':
        click_on_character()
    else:
        print("Acción no reconocida.")

def interpret_and_execute_gpt_response(gpt_response):
    action = parse_gpt_response_to_action(gpt_response)
    execute_action(action)

def parse_gpt_response_to_action(gpt_response):
    if 'recoge recursos' in gpt_response:
        return 'collect_resources'
    elif 'busca mobs' in gpt_response:
        return 'search_mob'
    elif 'rastrea personaje' in gpt_response:
        return 'track_character'
    else:
        return 'unknown_action'

////////////////////
-game_api.py
////////////
-language_model.py:

from openai import OpenAI  # Asegúrate de que estás importando OpenAI correctamente
import env  # Asumiendo que tienes un archivo env con tu clave API

# Inicializa el cliente con la clave API
client = OpenAI(api_key=env.API_KEY)

def get_gpt_response(prompt):
    # Usa el cliente para crear una respuesta de chat con el modelo GPT-4 Turbo
    completion = client.completions.create(
        model='gpt-4-turbo',
        prompt=[
            {"role": "system", "content": "You are a character in a role-playing game."},
            {"role": "user", "content": prompt}
        ]
    )
    # Retorna solo el contenido del mensaje de la respuesta
    return completion.choices[0].message.content.strip()


///////////////////

mapLocation: tiene algunas carpetas que se crean para obtener imagenes del mapa
combat_check_imagesguarda unas imagenes last_debug_image.png, last_reference_image.png
también tiene una imagen que dice combbat_debug.png y tiene una referencia que solo aparece en modo combate y la usaba para validar si estábamos en modo combate o no.
también tiene otra imagen llamada map_coordinates.png puede que esta sea una imagen que guardamos para ver las coordinadas del mapa.

modules:
Modules tiene estos archivos.py:
chat_management.py: por ahora vacio,

combat.py:

import pyautogui as pg
import time
import cv2
import pytesseract
import numpy as np
from modules.navigation import change_map
from modules.image_processing import capture_screenshot, image_difference, capture_and_process_image
from config import COMBAT_MODE_REGION, DIRECTION_PATH_ABSTRUB_ZAAP,  MAP_LOCATION_DIR, WAIT_TIME, PLAYER_NAME, PLAYER_DATA_REGION, BOARD_REGION, GAME_SCREEN_REGION
from modules.image_processing import capture_map_coordinates, capture_combat_map_frame, detect_map_edges
import torch
from ultralytics import YOLO
import mss
import random

class_colors = {}


def check_combat_status():
    """Revisa si el bot está en combate, basado en la presencia de la barra de estado."""
    while True:
        status_bar_image_path = capture_status_bar()
        
        # Aquí podrías realizar un procesamiento de imagen adicional si es necesario
        # Por ejemplo, aplicar filtros, detección de bordes, etc.

        if is_status_bar_detected(status_bar_image_path):
            print("Combat is still active...")
            hover_and_detect_player_name(PLAYER_NAME)
            
            # Aquí podrías agregar lógica adicional basada en la información de la barra de estado
        else:
            print("Combat status check indicates combat has ended.")
            break

        time.sleep(3) 
        
def is_status_bar_detected(image_path):
    image = cv2.imread(image_path)
    hsv = cv2.cvtColor(image, cv2.COLOR_BGR2HSV)
    
    # Rangos HSV para colores específicos
    # Estos valores son solo ejemplos, debes ajustarlos según los colores exactos de tu juego
    red_lower = np.array([0, 70, 50])
    red_upper = np.array([10, 255, 255])
    blue_lower = np.array([110, 50, 50])
    blue_upper = np.array([130, 255, 255])
    
    # Crear máscaras para los colores
    mask_red = cv2.inRange(hsv, red_lower, red_upper)
    mask_blue = cv2.inRange(hsv, blue_lower, blue_upper)
    
    # Buscar contornos en las máscaras
    contours_red, _ = cv2.findContours(mask_red, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)
    contours_blue, _ = cv2.findContours(mask_blue, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)
    
    # Dibujar contornos sobre la imagen original para la visualización
    cv2.drawContours(image, contours_red, -1, (0, 255, 0), 3)  # Dibuja contornos rojos en verde
    cv2.drawContours(image, contours_blue, -1, (255, 0, 0), 3)  # Dibuja contornos azules en azul

    # TESTEO :Mostrar las máscaras y la imagen original con contornos QUITAR!
    # cv2.imshow('Red Mask', mask_red)
    # cv2.imshow('Blue Mask', mask_blue)
    # cv2.imshow('Detected Contours', image)

    # cv2.waitKey(0)
    # cv2.destroyAllWindows()

    # Verifica si hay contornos significativos de los colores rojo y azul
    red_detected = any(cv2.contourArea(contour) > 100 for contour in contours_red)
    blue_detected = any(cv2.contourArea(contour) > 100 for contour in contours_blue)
    
    # Aquí podría ir la lógica adicional para el reconocimiento de patrones y formas

    return red_detected and blue_detected    

def hover_and_detect_player_name(player_name):
    
    
           # TODO: Identificar enemigo y dibujar línea
            # TODO: Validar casillas verdes de movimiento
              # TODO: Implementar y llamar a la función para el botón verde
            # green_button_data = detect_green_button_text(GREEN_BUTTON_REGION)
            # combat_data.append(green_button_data)

            # TODO: Implementar y llamar a la función para el campo de texto general
            # general_text_data = extract_general_text_field(GENERAL_TEXT_FIELD_REGION)
            # combat_data.append(general_text_data)
    """
    Mueve el cursor sobre los personajes en la barra y espera a que aparezcan los nombres en toda la pantalla.
    Mientras está en hover, recopila datos del jugador, del botón verde y del campo de texto general.
    """
    print("Buscando personaje y recopilando datos...")
    combat_data = []
    start_x, start_y, width, height = COMBAT_MODE_REGION
    num_steps = 16

    for step in range(num_steps - 1, -1, -1):
        # Mover el cursor en la barra lateral de COMBAT_MODE_REGION para activar el nombre
        hover_position = (start_x + width // 2, start_y + step * (height // num_steps))
        pg.moveTo(hover_position)
        time.sleep(0.2)  # Tiempo para que el UI responda al hover

        # Captura de BOARD_REGION después de cada movimiento de hover para detectar nombres en el tablero
        board_screenshot = pg.screenshot(region=BOARD_REGION)
        current_frame = np.array(board_screenshot)

        # Utilizar pytesseract para realizar OCR en la imagen capturada de BOARD_REGION
        ocr_result = pytesseract.image_to_data(current_frame, output_type=pytesseract.Output.DICT)
        for i, text in enumerate(ocr_result['text']):
            if player_name in text:
                x, y, w, h = ocr_result['left'][i], ocr_result['top'][i], ocr_result['width'][i], ocr_result['height'][i]
                
                # Dibujar la línea vertical 100 píxeles abajo del texto detectado
                draw_vertical_line(current_frame, x + w // 2, y + h , 20)
                
                # Guardar la imagen con la línea para verificación
                verification_image_path = f"{MAP_LOCATION_DIR}/verification_image_with_line.png"
                cv2.imwrite(verification_image_path, current_frame)
                print(f"Imagen con línea guardada para verificación en {verification_image_path}")
                
                cv2.imshow("Verification Image with Line", current_frame)
                cv2.waitKey(0)
                cv2.destroyAllWindows()

                # Agregar los datos recopilados al conjunto de datos de combate
                player_data = extract_player_data(PLAYER_DATA_REGION)
                combat_data.append(player_data)
                return combat_data  # Retorna después de detectar el primer nombre coincidente

        time.sleep(0.5)  # Pequeña pausa para evitar sobrecargar el sistema
        
def capture_status_bar():
    # Define la región donde se espera que esté la barra de estado de los personajes.
    # Estas coordenadas son hipotéticas y deben ser ajustadas según tu juego.
    status_bar_region = COMBAT_MODE_REGION
    status_bar_image = pg.screenshot(region=status_bar_region)
    status_bar_image_path = f"{MAP_LOCATION_DIR}/current_status_bar.png"
    status_bar_image.save(status_bar_image_path)
    return status_bar_image_path
        
def searchMob():
     print("Comenzando ruta de abstrub para ardilla.")
            # Aquí iría la lógica para buscar mobs en la zona.
     coordinates_before_change = capture_map_coordinates()
     print(f"Coordenadas antes de cambiar de mapa: {coordinates_before_change}")

     change_map(DIRECTION_PATH_ABSTRUB_ZAAP)
     time.sleep(WAIT_TIME)  # Espera después de intentar cambiar de mapa.

     coordinates_after_change = capture_map_coordinates()
     print(f"Coordenadas después de intentar cambiar de mapa: {coordinates_after_change}")

     if coordinates_before_change == coordinates_after_change:
         print("No se detectaron cambios en la posición del mapa. Verificando modo de combate...")
         check_combat_status()  # Verifica continuamente si está en combate
     else:
         print("El cambio de mapa fue exitoso.")
            # Ejemplo: find_mobs()

def initiate_combat_sequence():
    # Asegúrate de que estás en combate
    if check_combat_status():
        
        
        # Captura el marco del mapa de combate y detecta los bordes
        combat_map_frame = capture_combat_map_frame()
        combat_map_edges = detect_map_edges(combat_map_frame)
        
        cv2.imshow('Combat Map Edges', combat_map_edges)
        cv2.waitKey(0)
        cv2.destroyAllWindows()
        # Con base en los bordes, determina la posición del personaje y enemigos
        # ... (lógica para determinar posiciones)
        # Realiza el siguiente movimiento o ataque
        # ... (lógica para realizar acciones)

# Puedes llamar a initiate_combat_sequence en el lugar apropiado donde manejas el combate en main.py o combat.py

def gather_combat_data(player_name, player_data_region, green_button_region, general_text_field_region):
    """Recopila todos los datos necesarios durante el combate."""
    player_data = extract_player_data(player_data_region)
    green_button_data = detect_green_button_text(green_button_region)
    general_text_data = extract_general_text_field(general_text_field_region)
    
    # Retornar un diccionario con todos los datos recopilados
    return {
        "player_data": player_data,
        "green_button_data": green_button_data,
        "general_text_data": general_text_data
    }
    
def extract_player_data(player_data_region):
    """Extrae los datos del jugador de una región específica utilizando la función genérica."""
    player_data_text, player_data_image_path = capture_and_process_image(
        player_data_region, "player_data", "player_data_folder"
    )
    print("Datos del Jugador Extraídos:", player_data_text)
    return {"player_data": player_data_text, "image_path": player_data_image_path}

def detect_green_button_text(green_button_region):
    """Detecta el texto del botón verde en combate."""
    green_button_text, green_button_image_path = capture_and_process_image(
        green_button_region, "green_button", "green_button_folder"
    )
    return {"green_button_text": green_button_text, "image_path": green_button_image_path}

def extract_general_text_field(general_text_field_region):
    """Extrae el texto del campo general de texto en combate."""
    general_text, general_text_image_path = capture_and_process_image(
        general_text_field_region, "general_text", "general_text_folder"
    )
    return {"general_text": general_text, "image_path": general_text_image_path}


def draw_vertical_line(image, center_x, base_y, line_height):
    """Dibuja una línea vertical corta en la posición x del personaje detectado, empezando desde base_y hacia abajo."""
    adjusted_base_y = base_y  +70
    cv2.line(image, (center_x, adjusted_base_y), (center_x, adjusted_base_y - line_height), (0, 255, 0), 2)  
    return image


def get_color_for_class(class_name):
    if class_name not in class_colors:
        class_colors[class_name] = (random.randint(0, 255), random.randint(0, 255), random.randint(0, 255))
    return class_colors[class_name]

def detect_objects_in_real_time():
    # Cargar el modelo entrenado
    model = YOLO("C:\\Users\\Daniel\\Desktop\\Daniel\\labellmg\\datasetCombat\\runs\\detect\\train2\\weights\\best.pt")

    # Inicializar el capturador de pantalla
    sct = mss.mss()
    last_detections = {}
    screenshot_taken = False  # Flag para verificar si el pantallazo ya fue tomado

    while True:
        # Capturar la pantalla
        screenshot = sct.grab(GAME_SCREEN_REGION)
        img = np.array(screenshot)

        # Convertir la imagen de BGR a RGB (mss devuelve una imagen en formato BGR)
        img = cv2.cvtColor(img, cv2.COLOR_BGRA2BGR)

        # Hacer predicciones
        results = model(img)

        current_detections = {}
        log_entries = []

        # Dibujar las predicciones en la imagen
        for result in results[0].boxes.data:  # Acceder a los resultados como tensores
            x1, y1, x2, y2, conf, cls = result
            x1, y1, x2, y2 = map(int, [x1, y1, x2, y2])
            class_name = model.names[int(cls)]
            color = get_color_for_class(class_name)
            x_center, y_center = (x1 + x2) // 2, (y1 + y2) // 2

            current_detections[class_name] = (x_center, y_center, conf)

            # Solo dibujar si la posición ha cambiado significativamente
            if class_name not in last_detections or (x_center, y_center) != last_detections[class_name][:2]:
                cv2.rectangle(img, (x1, y1), (x2, y2), color, 2)
                cv2.putText(img, f'{class_name} {conf:.2f}', (x1, y1 - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, color, 2)

            # Preparar la entrada de log
            log_entries.append(f"- {class_name} at ({x_center}, {y_center}) with confidence {conf:.2f}")

        last_detections = current_detections

        # Imprimir los logs formateados
        if log_entries:
            print("\nDetected objects:")
            print("\n".join(log_entries))

        # Tomar y guardar el pantallazo solo una vez
        if not screenshot_taken and current_detections:
            cv2.imwrite("screenshot.png", img)
            screenshot_taken = True
            print("Pantallazo guardado como screenshot.png")

        # Pausa de 3 segundos
        time.sleep(3)

//////////////
characters_tracking.py:

import os
import cv2
import pyautogui as pg
from modules.image_processing import  capture_current_game_frame


CHARACTER_IMAGES_DIR = os.path.join("ojoIA", "characters", "main_character", "Static")

def load_character_templates():
    templates = {}
    for filename in os.listdir(CHARACTER_IMAGES_DIR):
        path = os.path.join(CHARACTER_IMAGES_DIR, filename)
        template = cv2.imread(path, cv2.IMREAD_COLOR)
        direction = os.path.splitext(filename)[0]  # 'look_down', 'look_up', etc.
        templates[direction] = template
    return templates

def detect_character(frame, templates):
    match_threshold = 0.8  # Ejemplo de umbral, debería ser ajustado según las pruebas
    for direction, template in templates.items():
        result = cv2.matchTemplate(frame, template, cv2.TM_CCOEFF_NORMED)
        min_val, max_val, min_loc, max_loc = cv2.minMaxLoc(result)
        if max_val > match_threshold:
            top_left = max_loc
            h, w = template.shape[:2]
            bottom_right = (top_left[0] + w, top_left[1] + h)
            center = (top_left[0] + w // 2, top_left[1] + h // 2)
            cv2.rectangle(frame, top_left, bottom_right, color=(0, 255, 0), thickness=2)
            return center  # Devuelve la posición central del personaje
    return None

def click_on_character(center):
    if center is not None:
        pg.click(center)
        
def track_and_click_character():
    templates = load_character_templates()
    frame = capture_current_game_frame()  # Necesitarás implementar esta función
    character_center = detect_character(frame, templates)
    if character_center is not None:
        print(f"Personaje detectado en {character_center}. Haciendo clic en el personaje...")
        click_on_character(character_center)
    else:
        print("Personaje no detectado.")


//////////////

import pytesseract
import os
import numpy as np
import cv2
import pyautogui as pg
from config import SCREENSHOTS_DIR, TESSERACT_CMD_PATH, MAP_LOCATION_DIR
from PIL import Image, ImageChops, UnidentifiedImageError



# Configura la ubicación de Tesseract en tu sistema
pytesseract.pytesseract.tesseract_cmd = TESSERACT_CMD_PATH


def capture_screenshot(region, filename, directory=SCREENSHOTS_DIR):
    """Toma una captura de pantalla de la región especificada y guarda la imagen en el directorio dado."""
    try:
        # Crea el directorio si no existe
        if not os.path.exists(directory):
            os.makedirs(directory)

        screenshot = pg.screenshot(region=region)
        screenshot_path = f"{directory}/{filename}"
        screenshot.save(screenshot_path)
        return screenshot_path
    except Exception as e:
        print(f"Error al capturar la pantalla: {e}")
        return None


def capture_map_coordinates():
    """Captura las coordenadas del mapa usando OCR."""
    try:
        region_to_capture = (0, 65, 100, 40)
        filename = "map_coordinates.png"
        screenshot_path = capture_screenshot(region_to_capture, filename, MAP_LOCATION_DIR)
        if screenshot_path:
            coordinates = process_image_for_text(screenshot_path)
            if coordinates.strip() == "":  # Verifica si Tesseract devolvió una cadena vacía.
                print("Tesseract no pudo detectar coordenadas en la imagen.")
                return "Unknown"  # Puedes devolver un valor que denote desconocido o error.
            return coordinates
        else:
            print("Fallo al capturar la imagen para OCR.")
            return "Capture Failed"
    except Exception as e:
        print(f"Error al capturar las coordenadas del mapa: {e}")
        return "Error"

def process_image_for_text(image_path):
    """Utiliza OCR para extraer texto de una imagen con configuraciones optimizadas."""
    try:
        image = Image.open(image_path)
        # Preprocesamiento de la imagen (opcional): puedes aplicar filtros para mejorar el contraste, convertir a escala de grises, etc.
        # ...
        # Configuraciones de Tesseract
        custom_config = r'--oem 3 --psm 6'
        return pytesseract.image_to_string(image, config=custom_config)
    except UnidentifiedImageError as e:
        print(f"No se pudo abrir la imagen: {e}")
        return None
    except Exception as e:
        print(f"Error al procesar la imagen para obtener texto: {e}")
        return None

def capture_and_process_image(region, image_name, output_folder):
    """Captura una imagen de una región específica, la guarda y la procesa con OCR para extraer texto."""
    # Comprueba y crea la carpeta de salida si no existe
    output_path = f"{MAP_LOCATION_DIR}/{output_folder}"
    if not os.path.exists(output_path):
        os.makedirs(output_path)
    
    # Captura y guarda la imagen
    image_path = f"{output_path}/{image_name}.png"
    image = pg.screenshot(region=region)
    image.save(image_path)

    # Procesar la imagen con OCR
    text = pytesseract.image_to_string(cv2.imread(image_path))
    return text, image_path

def get_image_difference(image1_path, image2_path):
    """Obtiene la diferencia entre dos imágenes."""
    try:
        img1 = Image.open(image1_path)
        img2 = Image.open(image2_path)
        return ImageChops.difference(img1, img2)
    except Exception as e:
        print(f"Error al obtener la diferencia entre imágenes: {e}")
        return None
    
def image_difference(image1_path, image2_path, save_debug=True):
    """Calcula la diferencia entre dos imágenes usando OpenCV y devuelve si son significativamente diferentes.
    Opcionalmente guarda las imágenes comparadas para depuración."""
    debug_dir = f"{MAP_LOCATION_DIR}/combat_check_images"  # Directorio para guardar imágenes de depuración
    if save_debug and not os.path.exists(debug_dir):
        os.makedirs(debug_dir)
    
    try:
        # Cargar imágenes y convertirlas a escala de grises
        img1 = cv2.imread(image1_path, cv2.IMREAD_GRAYSCALE)
        img2 = cv2.imread(image2_path, cv2.IMREAD_GRAYSCALE)

        # Calcular la diferencia entre las imágenes
        diff = cv2.absdiff(img1, img2)
        _, thresh = cv2.threshold(diff, 30, 255, cv2.THRESH_BINARY)

        # Guardar imágenes para depuración
        if save_debug:
            cv2.imwrite(os.path.join(debug_dir, 'last_reference_image.png'), img1)
            cv2.imwrite(os.path.join(debug_dir, 'last_debug_image.png'), img2)
            cv2.imwrite(os.path.join(debug_dir, 'diff_image.png'), thresh)  # Guardar imagen de diferencia para revisión

        # Comprobar si hay diferencias significativas
        if np.sum(thresh) > 0:  # Si hay píxeles blancos en la imagen umbralizada, hay diferencias
            print("Differences detected. Combat has ended or the scene has changed.")
            return True
        else:
            print("No differences found. Combat is still active.")
            return False
    except Exception as e:
        print(f"Error processing image difference using OpenCV: {e}")
        return True  # Si hay un error, suponer que el combate ha terminado por precaución

def capture_current_game_frame():
    """
    Captura la pantalla actual del juego y devuelve una imagen.
    La región capturada puede ser ajustada según la resolución y la ventana del juego.
    """
    game_region = (0, 0, 1920, 1080)  # Ajusta esta tupla a la región exacta del juego
    frame = pg.screenshot(region=game_region)
    frame = cv2.cvtColor(np.array(frame), cv2.COLOR_RGB2BGR)
    return frame

def detect_map_edges(frame):
    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)
    blurred = cv2.GaussianBlur(gray, (5, 5), 0)
    edges = cv2.Canny(blurred, 50, 150)
    return edges

def capture_combat_map_frame():
    # Definir la región específica donde se espera que esté el mapa de combate.
    # Ajusta los valores de la tupla según la posición y tamaño del mapa en tu juego.
    combat_map_region = (100, 200, 800, 600)
    frame = pg.screenshot(region=combat_map_region)
    frame = cv2.cvtColor(np.array(frame), cv2.COLOR_RGB2BGR)
    return frame


//////////////

navigation.py:
import pyautogui as pg
import time
from config import TOOLTIP_REGIONS, WAIT_TIME

CURRENT_DIRECTION_INDEX = 0

def change_map(path):
    global CURRENT_DIRECTION_INDEX
    direction_to_move = path[CURRENT_DIRECTION_INDEX]
    CURRENT_DIRECTION_INDEX = (CURRENT_DIRECTION_INDEX + 1) % len(path)
    
    # Mueve el cursor al botón de la dirección deseada y hace clic.
    pg.click(TOOLTIP_REGIONS[direction_to_move])
    time.sleep(WAIT_TIME)  # Espera después de clickear para dar tiempo al juego de cambiar el mapa.
    
    # Aquí podrías implementar lógica adicional si es necesario, como verificaciones después de cambiar el mapa.

//////////////
resource_management.py:
import pyautogui as pg
from modules.combat import check_combat_status
from modules.navigation import change_map
from modules.image_processing import capture_map_coordinates
import time
from config import RESOURCE_PATHS, CONFIDENCE_LEVEL, WAIT_TIME, RESOURCES_TYPE,DIRECTION_PATH_ESCARAHOJA_ZAAP,DIRECTION_PATH_ROBLE

EXCEPTIONS = {
        'fresno': {
            'ignored_positions': [(1465, 862)],  # Lista de posiciones a ignorar
        },
        'trigo': {
            'special_click_offsets': [(907, 588, 4, 0)],  # (x, y, offset_x, offset_y) donde x, y es la posición a buscar y (offset_x, offset_y) es cuánto mover el mouse antes de clickear
        }
    }


def find_resource_on_screen(resource_type, category):
    """Busca los recursos en pantalla por tipo y categoría y devuelve la ubicación si los encuentra."""
    category_dict = RESOURCE_PATHS.get(category, {})
    paths = category_dict.get(resource_type, [])
    for path in paths:
        try:
            location = pg.locateCenterOnScreen(path, confidence=CONFIDENCE_LEVEL)
            if location:
                print(f"Recurso de tipo {resource_type} en categoría {category} encontrado en {path}.")
                return location
        except pg.ImageNotFoundException:
            continue  # Simplemente continúa con la siguiente imagen
        except Exception as e:
            print(f"Error al buscar el recurso de tipo {resource_type} en categoría {category} en {path}: {e}")
    # Si termina el bucle y no encuentra nada, imprime un mensaje general.
    print(f"Recurso de tipo {resource_type} en categoría {category} no encontrado.")
    return None



def apply_exceptions(resource_type, location):
    """Aplica las excepciones de clic especificadas en el diccionario EXCEPTIONS."""
    if resource_type in EXCEPTIONS:
        resource_exceptions = EXCEPTIONS[resource_type]
        print(f"Aplicando excepciones para {resource_type}, posición encontrada: {location}")

        # Para el caso de trigo que necesita un clic especial
        if 'special_click_offsets' in resource_exceptions:
            for (x, y, offset_x, offset_y) in resource_exceptions['special_click_offsets']:
                if (location.x, location.y) == (x, y):
                    new_location = (location.x + offset_x, location.y + offset_y)
                    print(f"Aplicando offset especial a {resource_type}: {new_location}")
                    return new_location

        # Para el caso de fresno ignorado
        if 'ignored_positions' in resource_exceptions:
            for ignored_pos in resource_exceptions['ignored_positions']:
                if (location.x, location.y) == ignored_pos:
                    print(f"Ignorando posición de {resource_type} en {ignored_pos}")
                    return None

        # Para el caso de salvia que necesita ajustar el clic
        if 'click_adjustments' in resource_exceptions:
            for (offset_x, offset_y) in resource_exceptions['click_adjustments']:
                new_location = (location.x + offset_x, location.y + offset_y)
                print(f"Aplicando ajuste de clic a {resource_type}: {new_location}")
                return new_location

    return (location.x, location.y)

    """Intenta recolectar un recurso dado si se encuentra en pantalla."""
    location = find_resource_on_screen(resource_type)
    if location:
        # Aplica las excepciones antes de hacer clic.
        new_location = apply_exceptions(resource_type, location)
        if new_location:
            pg.click(new_location)
            time.sleep(WAIT_TIME)
            print(f"Recolectado {resource_type}.")
            return True
        else:
            print(f"Recurso {resource_type} ignorado debido a una excepción.")
    return False
def collect_resource(resource_type):
    """Intenta recolectar un recurso dado si se encuentra en pantalla."""
    category = RESOURCES_TYPE.get(resource_type, None)
    if category is None:
        print(f"Categoría no definida para el recurso {resource_type}.")
        return False  # Salir si no hay categoría definida.

    location = find_resource_on_screen(resource_type, category)
    if location:
        # Aplica las excepciones antes de hacer clic.
        new_location = apply_exceptions(resource_type, location)
        if new_location:
            pg.click(new_location)
            time.sleep(WAIT_TIME)
            print(f"Recolectado {resource_type}.")
            return True
        else:
            print(f"Recurso {resource_type} ignorado debido a una excepción.")
    return False
def search_and_collect_resources():
    """Bucle principal para buscar y recolectar recursos repetidamente hasta que no encuentre más."""
    resources_found = False
    for resource_type in RESOURCES_TYPE:  # Solo iterar sobre los tipos definidos con categoría
        while collect_resource(resource_type):  # Continúa intentando recolectar mientras haya recursos.
            resources_found = True
            print(f"Recolectado {resource_type}. Volviendo a buscar {resource_type}...")
    
    if not resources_found:
        print("No se encontraron más recursos. Intentando cambiar de mapa...")
        return False
    return True

def search_resources():
        resources_collected = search_and_collect_resources()
        if not resources_collected:
            print("No se encontraron recursos. Intentando cambiar de mapa...")
            coordinates_before_change = capture_map_coordinates()
            print(f"Coordenadas antes de cambiar de mapa: {coordinates_before_change}")

            change_map(DIRECTION_PATH_ROBLE)
            time.sleep(WAIT_TIME)  # Espera después de intentar cambiar de mapa.

            coordinates_after_change = capture_map_coordinates()
            print(f"Coordenadas después de intentar cambiar de mapa: {coordinates_after_change}")

            if coordinates_before_change == coordinates_after_change:
                print("No se detectaron cambios en la posición del mapa. Verificando modo de combate...")
                check_combat_status()  # Verifica continuamente si está en combate
            else:
                print("El cambio de mapa fue exitoso.")
        time.sleep(2)  # Pequeña pausa antes de mostrar de nuevo el menú



//////////////
ojoIAtiene resources, donde extraemos las imagenes de los recursos para que con las librerias comparemos y demos click de estar en la Lista
y otras imagenes como
combat_indicator.pngcombat_status.indicator.PNG

ojolA
│
├───characters
│   │
│   └───main_character
│       │
│       └───Static
│             ├─ look_down.png (17 KB)
│             ├─ look_down_left.png (10.4 KB)
│             ├─ look_down_right.png
│             ├─ look_left.png
│             ├─ look_right.png
│             ├─ look_up.png
│             ├─ look_up_left.png
│             └─ look_up_right.png
│
└───resources
    │
    ├───cereals
    │
    ├───herbage
    │
    ├───minerals
    │
    └───wood

//////////////
una carpeta llamda test, con  unos archivos vacios de py
test_combat.py
test_image_processing.py
test_navigation.py

//////////////

Una carpeta llamada utils que tiene

file_managment.py:
import os
from config import MAP_LOCATION_DIR

def save_data_to_file(data, filename):
    """Guarda datos en un archivo específico."""
    with open(os.path.join(MAP_LOCATION_DIR, filename), 'w') as file:
        # Aquí iría la lógica para escribir datos en el archivo.
        pass

def read_data_from_file(filename):
    """Lee datos de un archivo."""
    with open(os.path.join(MAP_LOCATION_DIR, filename), 'r') as file:
        # Aquí iría la lógica para leer datos del archivo.
        pass
y los otros vacios:
helpers.py y logger.py
//////////////

al final tenemos el config.py:
import pytesseract
import os
# Configuración de la ruta de Tesseract-OCR
TESSERACT_CMD_PATH = r'C:\Program Files\Tesseract-OCR\tesseract.exe'
pytesseract.pytesseract.tesseract_cmd = TESSERACT_CMD_PATH

# Directorios para el manejo de recursos e imágenes
MAP_LOCATION_DIR = "mapLocation"
SCREENSHOTS_DIR = "ojoIA"
RESOURCES_DIR = "resources"
RESOURCES_TYPE =  {
    "trigo": "cereals",
    "castano": "wood",
    "roble": "wood",
    "fresno": "wood",
    "nogal": "wood",
    "hierro": "minerals",
    "ortiga": "herbage",
    "salvia": "herbage"
}
#region
REGION_TO_CAPTURE = (0, 65, 100 , 90-50) 
GAME_SCREEN_REGION = {"top": 0, "left": 0, "width": 1920, "height": 580}



# Configuraciones de imágenes y navegación
IMAGE_OFFSET = 25
WAIT_TIME = 6
SCREENSHOT_SIZE = 100

DIRECTION_PATH_ABSTRUB_ZAAP = [
    'left', 'left', 'down', 'right', 'right', 'right', 'up', 'up', 'left', 'left',
    'left', 'down', 'left', 'left', 'down', 'right', 'down', 'left', 'right', 'right',
    'up', 'right', 'down', 'right', 'up','right','down','right','up','up', 'up','up', 'up',
    'left','down','left','up','left','down','left','up','left','left','down','right','down','left','down',
]

DIRECTION_PATH_ESCARAHOJA_ZAAP = [
    'right', 'right', 'up', 'up', 'right', 'up', 'up', 'right', 'down', 'down',
    'down', 'up', 'up', 'right', 'right', 'up', 'right', 'up', 'up', 'left',
    'up', 'right', 'up', 'left'
]
#-11 ,-8
DIRECTION_PATH_ROBLE = [
    'right', 'up', 'right', 'down', 'right', 'up', 
    'up', 'up', 'up', 'up', 'right', 'up', 'left', 'up', 
    'right', 'up', 'left', 'up', 'left', 'left',
    'left', 'left', 'left', 'left', 'left', 'down', 'down', 'left', 
    'down', 'right', 'down', 'down', 'down', 'right', 'right', 'down','down','down', 'right'
]

# Rutas a imágenes específicas de recursos
RESOURCE_PATHS = {
    "cereals": {
        "trigo": [
            "ojoIA/resources/cereals/trigo1.PNG",
            "ojoIA/resources/cereals/trigo2.PNG",
            "ojoIA/resources/cereals/trigo3.PNG",
            "ojoIA/resources/cereals/trigo4.PNG",
            "ojoIA/resources/cereals/trigo5.PNG"
        ],
         "cebada": [
            "ojoIA/resources/cereals/cebada1.PNG",
            "ojoIA/resources/cereals/cebada2.PNG",

        ]
    },
    "wood": {
        "castano": [
            "ojoIA/resources/wood/casta1.PNG",
            "ojoIA/resources/wood/casta2.PNG",
            "ojoIA/resources/wood/casta3.PNG",
            "ojoIA/resources/wood/casta4.PNG"
        ],
        "fresno": [
            "ojoIA/resources/wood/fresno1.PNG",
            "ojoIA/resources/wood/fresno2.PNG",
            "ojoIA/resources/wood/fresno3.PNG"
        ],
        "nogal": [
            "ojoIA/resources/wood/nogal1.PNG",
            "ojoIA/resources/wood/nogal2.PNG",
            "ojoIA/resources/wood/nogal3.PNG"
        ],
        "roble": [
            "ojoIA/resources/wood/roble1.PNG",
            "ojoIA/resources/wood/roble2.PNG",
        ]
    },
    
    #"minerals": {
    #    "hierro": [
    #        "ojoIA/resources/minerals/hierro1.PNG",
    #        "ojoIA/resources/minerals/hierro2.PNG",
    #        "ojoIA/resources/minerals/hierro3.PNG",
    #        "ojoIA/resources/minerals/hierro4.PNG",
    #        "ojoIA/resources/minerals/hierro5.PNG"
    #    ]
    # }
    
    "herbage": {
        "ortiga": [
            "ojoIA/resources/herbage/ortiga1.PNG",
            "ojoIA/resources/herbage/ortiga2.PNG",
            "ojoIA/resources/herbage/ortiga3.PNG",
            "ojoIA/resources/herbage/ortiga4.PNG",
            "ojoIA/resources/herbage/ortiga5.PNG"
        ],
        "salvia": [
            "ojoIA/resources/herbage/salvia1.PNG",
            "ojoIA/resources/herbage/salvia2.PNG"
        ]
    }
}


# Regiones de captura de pantalla para detectar tooltips y otros indicadores
TOOLTIP_REGIONS = {
    'up': (960 - 50, 0, 100, 50),
    'left': (0 + 300, 540 - 25, 100, 50),
    'right': (1920 - 400, 540 - 25, 100, 50),
    'down': (960 - 50, 1080 - 200, 100, 50),
}
#Combat 
COMBAT_MODE_REGION = (1610, 95, 155, 875)  # Conserva para operaciones de hover
BOARD_REGION = (300, 25, 1300, 900) 


CONFIDENCE_LEVEL = 0.7

#Datos de el personaje
PLAYER_NAME = 'Twenzen'
PLAYER_DATA_REGION = (715, 900, 120, 140) 

SCREENSHOT_DIR = os.path.join("ojoIA", "screenshots")


/////////////////
Y el centro de todo

main.py :
from modules.combat import searchMob, initiate_combat_sequence, detect_objects_in_real_time
from modules.resource_management import search_resources
from modules.image_processing import capture_map_coordinates, capture_current_game_frame
from modules.characters_tracking import load_character_templates, detect_character, click_on_character, track_and_click_character
from api.bot_controller import interpret_and_execute_gpt_response
from api.language_model import get_gpt_response
import inspect
import modules

def list_module_functions(module):
    functions_list = []
    for name, obj in inspect.getmembers(module):
        if inspect.isfunction(obj):
            functions_list.append(name)
    return functions_list

def explore_project():
    modules_list = [modules.combat, modules.resource_management, modules.characters_tracking]
    all_functions = {}
    for module in modules_list:
        module_name = module.__name__.split('.')[-1]
        all_functions[module_name] = list_module_functions(module)
    return all_functions

def main_menu():
    print("Seleccione una opción:")
    print("1. Recolectar recursos")
    print("2. Buscar mobs en la zona")
    print("3. Leer chat del juego (Funcionalidad en desarrollo)")
    print("4. Combate automático")
    print("5. Observa el entorno y obtiene datos")
    print("6. Interactuar con GPT-4")
    print("7. Explorar funciones del proyecto con GPT-4")
    print("8. Activar detección en tiempo real (YOLO)")

    choice = input("Introduce el número de la opción deseada: ")
    return choice

def interact_with_gpt4():
    user_prompt = input("Describe la misión o consulta para el personaje: ")
    gpt_response = get_gpt_response(user_prompt)
    print(f"GPT-4: {gpt_response}")
    interpret_and_execute_gpt_response(gpt_response)

def explore_with_gpt4():
    all_functions = explore_project()
    prompt = f"Estas son las funciones disponibles en el proyecto: {all_functions}. Sugiere acciones creativas que el bot pueda realizar utilizando estas funciones."
    gpt_response = get_gpt_response(prompt)
    print(f"GPT-4: {gpt_response}")
    interpret_and_execute_gpt_response(gpt_response)

def main():
    print("Iniciando el bot...")
    initial_coordinates = capture_map_coordinates()
    if initial_coordinates:
        print(f"Coordenadas iniciales del mapa al iniciar: {initial_coordinates}")
    else:
        print("No se pudieron capturar las coordenadas iniciales del mapa.")

    while True:
        user_choice = main_menu()

        if user_choice == '1':
            while True:
                search_resources()
        elif user_choice == '2':
            while True:
                searchMob()
        elif user_choice == '3':
            print("Esta funcionalidad aún está en desarrollo.")
        elif user_choice == '4':
            initiate_combat_sequence()
        elif user_choice == '5':
            track_and_click_character()
        elif user_choice == '6':
            interact_with_gpt4()
        elif user_choice == '7':
            explore_with_gpt4()
        elif user_choice == '8':
            detect_objects_in_real_time()
        else:
            print("Opción no válida. Por favor, intenta de nuevo.")

if __name__ == "__main__":
    main()